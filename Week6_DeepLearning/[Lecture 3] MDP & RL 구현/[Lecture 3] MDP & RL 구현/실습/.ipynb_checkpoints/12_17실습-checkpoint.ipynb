{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util, math, random\n",
    "from collections import defaultdict\n",
    "from util import ValueIteration\n",
    "\n",
    "_FILL_IN_ = None\n",
    "\n",
    "############################################################\n",
    "# Problem A\n",
    "\n",
    "class ExampleMDP(util.MDP):\n",
    "    def startState(self):\n",
    "        return 0\n",
    "\n",
    "    # Return set of actions possible from |state|.\n",
    "    def actions(self, state):\n",
    "        return ['Left', 'Right']\n",
    "\n",
    "    # Return a list of (newState, prob, reward) tuples corresponding to edges\n",
    "    # coming out of |state|.\n",
    "    def succAndProbReward(self, state, action):\n",
    "        if state == -2 or state == 2:\n",
    "            return []\n",
    "        \n",
    "        leftReward = -5\n",
    "        rightReward = -5\n",
    "\n",
    "        if state - 1 == -2:\n",
    "            leftReward = 20\n",
    "        if state + 1 == 2:\n",
    "            rightReward = 100\n",
    "        \n",
    "        if action == 'Left':\n",
    "            results = [(state-1, 0.8, leftReward), (state+1, 0.2, rightReward)]\n",
    "        elif  action == 'Right':\n",
    "            results = [(state-1, 0.7, leftReward), (state+1, 0.3, rightReward)]\n",
    "        else:\n",
    "            results = []\n",
    "        \n",
    "        return results\n",
    "            \n",
    "    def discount(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Problem C\n",
    "\n",
    "class BlackjackMDP(util.MDP):\n",
    "    def __init__(self, cardValues, multiplicity, threshold, peekCost):\n",
    "        \"\"\"\n",
    "        cardValues: array of card values for each card type\n",
    "        multiplicity: number of each card type\n",
    "        threshold: maximum total before going bust\n",
    "        peekCost: how much it costs to peek at the next card\n",
    "        \"\"\"\n",
    "        self.cardValues = cardValues\n",
    "        self.multiplicity = multiplicity\n",
    "        self.threshold = threshold\n",
    "        self.peekCost = peekCost\n",
    "\n",
    "    # Return the start state.\n",
    "    # Look at this function to learn about the state representation.\n",
    "    # The first element of the tuple is the sum of the cards in the player's\n",
    "    # hand.\n",
    "    # The second element is the index (not the value) of the next card, if the player peeked in the\n",
    "    # last action.  If they didn't peek, this will be None.\n",
    "    # The final element is the current deck.\n",
    "    def startState(self):\n",
    "        return (0, None, (self.multiplicity,) * len(self.cardValues))  # total, next card (if any), multiplicity for each card\n",
    "\n",
    "    # Return set of actions possible from |state|.\n",
    "    # You do not need to modify this function.\n",
    "    # All logic for dealing with end states should be done in succAndProbReward\n",
    "    def actions(self, state):\n",
    "        return ['Take', 'Peek', 'Quit']\n",
    "\n",
    "    # Return a list of (newState, prob, reward) tuples corresponding to edges\n",
    "    # coming out of |state|.  Indicate a terminal state (after quitting or\n",
    "    # busting) by setting the deck to None. \n",
    "    # When the probability is 0 for a particular transition, don't include that \n",
    "    # in the list returned by succAndProbReward.\n",
    "    def succAndProbReward(self, state, action):\n",
    "        # BEGIN_YOUR_CODE\n",
    "        succ_prob_reward_list = []\n",
    "        card_sum, peek_idx, deck = state  # card_sum = the sum of taken cards' values\n",
    "\n",
    "        if deck is None:  # when there is no card in the deck\n",
    "            pass          # no possible successor state\n",
    "\n",
    "        elif action == 'Take':\n",
    "            num_all_cards = sum(deck)  # the number of all cards\n",
    "\n",
    "            # get_succ_reward(idx) returns a successor state and a reward, when a card is taken.\n",
    "            def get_succ_reward(idx):\n",
    "                new_card_sum = _FILL_IN_  # HINT: use self.cardValues  # what's the new sum of card values, when we take a new card?\n",
    "                if new_card_sum > self.threshold:  # when the card sum exceeds the threshold\n",
    "                    new_deck = None\n",
    "                    reward = 0\n",
    "                elif num_all_cards > 1:  # sum(new_deck) > 0; when some cards remain\n",
    "                    new_deck = list(deck)\n",
    "                    _FILL_IN_  # decrease the number of instances of the taken card.\n",
    "                    new_deck = tuple(new_deck)\n",
    "                    reward = 0\n",
    "                else:  # when there is no card remaining\n",
    "                    new_deck = None\n",
    "                    reward = new_card_sum\n",
    "                succ = new_card_sum, None, new_deck\n",
    "                return succ, reward\n",
    "\n",
    "            # Peek implementation ----------------------------------------\n",
    "            if peek_idx is not None:  # when previous action was 'Peek'\n",
    "                succ, reward = get_succ_reward(peek_idx)\n",
    "                succ_prob_reward_list.append((succ, 1, reward))\n",
    "            # ---------------------------------------- Peek implementation\n",
    "            else:  # when previous action was not 'Peek'\n",
    "                for idx, num in enumerate(deck):\n",
    "                    if num == 0:\n",
    "                        continue                        \n",
    "                    succ, reward = get_succ_reward(idx)\n",
    "                    prob = _FILL_IN_\n",
    "                    succ_prob_reward_list.append((succ, prob, reward))\n",
    "\n",
    "        # Peek implementation ----------------------------------------\n",
    "        elif action == 'Peek':\n",
    "            if peek_idx is None:\n",
    "                num_all_cards = sum(deck)\n",
    "\n",
    "                for idx, num in enumerate(deck):\n",
    "                    if num == 0:\n",
    "                        continue\n",
    "                    prob = _FILL_IN_\n",
    "                    succ_prob_reward_list.append((_FILL_IN_, prob, - self.peekCost))  # HINT: has the form (new_card_sum, new_peek_idx, new_deck)\n",
    "        # ---------------------------------------- Peek implementation\n",
    "\n",
    "        elif action == 'Quit':\n",
    "            succ_prob_reward_list.append(((card_sum, None, None), 1, card_sum))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Undefined action '{}'\".format(action))\n",
    "\n",
    "        return succ_prob_reward_list\n",
    "        # END_YOUR_CODE\n",
    "\n",
    "    def discount(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "# Problem D: Q learning\n",
    "\n",
    "# Performs Q-learning.  Read util.RLAlgorithm for more information.\n",
    "# actions: a function that takes a state and returns a list of actions.\n",
    "# discount: a number between 0 and 1, which determines the discount factor\n",
    "# featureExtractor: a function that takes a state and action and returns a list of (feature name, feature value) pairs.\n",
    "# explorationProb: the epsilon value indicating how frequently the policy\n",
    "# returns a random action\n",
    "class QLearningAlgorithm(util.RLAlgorithm):\n",
    "    def __init__(self, actions, discount, featureExtractor, explorationProb=0.2):\n",
    "        self.actions = actions\n",
    "        self.discount = discount\n",
    "        self.featureExtractor = featureExtractor\n",
    "        self.explorationProb = explorationProb\n",
    "        self.weights = defaultdict(float)\n",
    "        self.numIters = 0\n",
    "\n",
    "    # Return the Q function associated with the weights and features\n",
    "    def getQ(self, state, action):\n",
    "        score = 0\n",
    "        for f, v in self.featureExtractor(state, action):\n",
    "            score += self.weights[f] * v\n",
    "        return score\n",
    "\n",
    "    # This algorithm will produce an action given a state.\n",
    "    # Here we use the epsilon-greedy algorithm: with probability\n",
    "    # |explorationProb|, take a random action.\n",
    "    def getAction(self, state):\n",
    "        self.numIters += 1\n",
    "        if random.random() < self.explorationProb:\n",
    "            return random.choice(self.actions(state))\n",
    "        else:\n",
    "            return max((self.getQ(state, action), action) for action in self.actions(state))[1]\n",
    "\n",
    "    # Call this function to get the step size to update the weights.\n",
    "    def getStepSize(self):\n",
    "        return 1.0 / math.sqrt(self.numIters)\n",
    "\n",
    "    # We will call this function with (s, a, r, s'), which you should use to update |weights|.\n",
    "    # Note that if s is a terminal state, then s' will be None.  Remember to check for this.\n",
    "    # You should update the weights using self.getStepSize(); use\n",
    "    # self.getQ() to compute the current estimate of the parameters.\n",
    "    def incorporateFeedback(self, state, action, reward, newState):\n",
    "        # BEGIN_YOUR_CODE\n",
    "        if newState is None:\n",
    "            v_opt = 0\n",
    "        else:\n",
    "            v_opt = max(self.getQ(newState, a) for a in self.actions(newState))  # v_opt(s')\n",
    "        diff = _FILL_IN_  # HINT: use self.getQ and self.discount\n",
    "        for f, v in self.featureExtractor(state, action):\n",
    "            eta = self.getStepSize()\n",
    "            self.weights[f] -= _FILL_IN_\n",
    "        # END_YOUR_CODE\n",
    "\n",
    "# Return a singleton list containing indicator feature for the (state, action)\n",
    "# pair.  Provides no generalization.\n",
    "def identityFeatureExtractor(state, action):\n",
    "    featureKey = (state, action)\n",
    "    featureValue = 1\n",
    "    return [(featureKey, featureValue)]\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "# Problem E: convergence of Q-learning\n",
    "\n",
    "def compareQLandVI(targetMDP, featureExtractor):\n",
    "    QL = QLearningAlgorithm(targetMDP.actions, 1, featureExtractor)\n",
    "    VI = ValueIteration()\n",
    "    \n",
    "    util.simulate(targetMDP, QL, numTrials=30000)\n",
    "    VI.solve(targetMDP)\n",
    "\n",
    "    diffPolicyStates = []\n",
    "    QL.explorationProb = 0\n",
    "    for state in targetMDP.states:\n",
    "        #print state, QL.getAction(state), VI.pi[state]\n",
    "        if QL.getAction(state) != VI.pi[state]:\n",
    "            diffPolicyStates.append(state)\n",
    "    print(\"%d/%d = %f%% different states\"%(len(diffPolicyStates), len(targetMDP.states), len(diffPolicyStates)/float(len(targetMDP.states))))\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "# Problem F: features for Q-learning.\n",
    "\n",
    "# You should return a list of (feature key, feature value) pairs (see\n",
    "# identityFeatureExtractor()).\n",
    "# Implement the following features:\n",
    "# - indicator on the total and the action (1 feature).\n",
    "# - indicator on the presence/absence of each card and the action (1 feature).\n",
    "#       Example: if the deck is (3, 4, 0 , 2), then your indicator on the presence of each card is (1,1,0,1)\n",
    "#       Only add this feature if the deck != None\n",
    "# - indicator on the number of cards for each card type and the action (len(counts) features).  Only add these features if the deck != None\n",
    "def blackjackFeatureExtractor(state, action):\n",
    "    total, nextCard, counts = state\n",
    "    # BEGIN_YOUR_CODE\n",
    "    type_1, type_2, type_3 = range(3)  # we have 3 types of features\n",
    "    features = []\n",
    "\n",
    "    # type 1\n",
    "    features.append((type_1, (total, action)))\n",
    "\n",
    "    if counts != None:\n",
    "        # type 2\n",
    "        features.append((type_2, (tuple(1 if count > 0 else 0 for count in counts), action)))\n",
    "\n",
    "        for idx, num in enumerate(counts):\n",
    "            # type 3\n",
    "            features.append((type_3, (idx, num, action)))\n",
    "\n",
    "    # all features have 1s as values\n",
    "    return [(feature, 1) for feature in features]\n",
    "    # END_YOUR_CODE\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
